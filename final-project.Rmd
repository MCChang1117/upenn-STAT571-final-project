---
title: "Housing Rental Pricing Prediction"
author:
- Pragyat Agrawal
- Ruxuan Ji
- Meng-Chuan Chang
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
  html_document:
    toc: yes
    theme: united
    toc_depth: 3
    number_sections: yes
  word_document:
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T, cache.lazy = FALSE) # notice cache=T here
knitr::opts_chunk$set(fig.height=4, fig.width=7, fig.align = 'center', warning = F)
if(!require('pacman')) {
  install.packages('pacman')
}

pacman::p_load(data.table, dplyr, tidyverse, xtable, ggplot2, glmnet, randomForest, tfdatasets, keras, car, partykit, pander)
```

# Executive Summary

## Background

The US rental market has been growing rapidly growing over time, making it one of the most sought after areas for investments. Be it from small home owners to big private equity firms, everyone seems to be after housing, expecting the values of these houses to rise and supplement their income by renting these places. Many people consider that location is the "only important" factor responsible for a house's value and the rent that can be expected of it, but this is far from the truth. There are a lot of other factors that need to be considered for determining housing valuations as we see a huge variation in prices in houses located in the same vicinity. There must be something about these houses which is causing such a big price change. Hence we will be analyzing the data related to US Rental Listings in Summer of 2021, to find which of these factors, which consist of many in-house amenity components, impacts housing values the most. 

These amenities range from simple aka micro-factors like the availability of Pools and Dishwashers in the house to major aka macro-factors i.e. cities. The data will also give us the opportunity to find in which cities are these factors playing the most impact. With over 27,000 values for each predictor in our data, we have a sufficient sample size to make a reasonable conclusion regarding the price of these summer rentals. 

Graph: Rental vacancy rates in the United States from 2000 to 2021, by region (Source: Statista.com) 

This shows how the US housing market has been more sought after year by year, making housing a form of valuable investment. This increase in demand has already pushed up the prices. 
```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("image/rental-vacancy-rates.png")
```

## Description of Data

The data is gathered from Kaggle, a vast community published code and data repository. This data was pulled from Rentler.com on 7/12/2021, 8/12/2021, and 9/6/2021, and population density data was scraped by zip code from mapszipcode.com on 7/12/2021. The pull from Rentler.com resulted in 4 CSV files, which included the main rental listing, the list of amenities, the list of lease terms, and who was responsible for paying each utility. Many of the sparsely populated variables were dropped before denormalizing the dataset. The rental listing information was joined with the population and population density information from mapszipcode.com (Source: Kaggle). Many of the data columns in the dataset are embedded in binary format, with 0 representing the absence of the predictor attribute while 1 shows that the attribute is present. The data file is massive at around half a Gigabyte. Working with this data size, we would have to use EDA or take a subset of the dataset for R to run effectively and not crash over the large size of the data. We will explore this idea in later sections. For now, the data is sufficient for analysis.

Our response variable is PRICE which represents the monthly price for the particular summer listing on rental.com.


\begin{table}[!h]
\centering
\caption{Variables and their descriptions}
\begin{tabular}{cc}
\hline 
Variable & Description\tabularnewline
\hline 
V1 & Numbering for the house number we are looking at (form of house identification)\tabularnewline
pool & Binary data to show if pool exists in this house (1) or not (0)\tabularnewline
dishwasher & Binary data to show if dishwasher exists in the house (1) or not (0)\tabularnewline
washer-dryer & Binary data to show if washer-dryer exists in the house(1) or not (0)\tabularnewline
ac & Binary data to show if air conditioning exists in the house (1) or not (0)\tabularnewline
parking & Binary data to show if Parking exists in the house (1) or not (0)\tabularnewline
zip & Zip code of the property\tabularnewline
price & Monthly rent price for the property\tabularnewline
city & City where the property is located\tabularnewline
num\_beds & Number of beds in the property\tabularnewline
num\_baths & Number of baths in the property\tabularnewline
house\_type & Type of house we are looking at\tabularnewline
sqft & Square Feet in the property\tabularnewline
smoking\_ind & Does the rental allow smoking (Yes/No) \tabularnewline
pets\_ind & Does the rental allow pets (Yes/No)\tabularnewline
acres & Number of acres rental includes\tabularnewline
description & 4000 character listing description of the rental\tabularnewline
ZipCity & Primary city for the zip code\tabularnewline
Population & Population in the zip code\tabularnewline
PopulationDensity & Population density per square mile for zip code\tabularnewline
security\_deposit & Security deposit required\tabularnewline
\hline 
\end{tabular}
\label{lyxtab2}
\end{table}

## Goal

This study aims to analyze the most critical factors affecting housing rental prices in the US. We will be using the variables in the dataset to do so. In our preliminary market analysis, we found that many factors determine rental prices, and hence we will try to ascertain factors that have a significant impact on rental prices. Our goal is to build a model that will give us the value added or subtracted from a house with/without the presence of a variable factor. This observation will benefit people looking to rent properties in the US and help them get a better value for the kind of place they may be looking for. 

## Summary of Findings

When we started the analysis, we initially thought that the rental prices would be affected by a very few factors that may push the results higher. But this is not true. Instead, there are a plethora of factors affecting housing rental prices. From our research, we found a lot of significant factors affecting housing like the availability of a dishwasher, washer-dryer, number of baths, population density, etc. Some factors were expected due to the monetary values associated with them. Still, some factors, like if smoking is allowed or not or pets are allowed or not, were very unanticipated to be significant. Hence, through this research, we pinpoint the various factors that go into play in determining housing prices. 


## Issues and Limitations

The biggest issue we initially faced was concerning the file size, which turned out to be quite massive, even for R Studio. The raw data we started with was half a gigabyte big, which was very massive for any form of extrapolation. Hence we had to shorten the data, choosing 20,000 data points for a particular seed to ensure reproducibility. After that, we also had some problems with knitting the data, because of which we had to keep clearing our knitr cache to ensure we get the correct data across on the PDF. Finally, the random forest part took some time as it had to run a data-heavy analysis. We had to wait for this part out as we wanted to give the highest quality output. 

# Exploratory Data Analysis

## Data Preprocessing/Cleaning

### Read the data

Firstly, we read the data from Kaggle - [US Rental Listings Summer 2021](https://www.kaggle.com/datasets/elizabethveillon/us-rental-listings-summer-2021)

```{r load, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
rental_data <- fread("data/Rental_Properties.csv")
summary(rental_data)
```

Then we read SOI Tax statistics in 2019 from [IRS](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2019-zip-code-data-soi)

```{r load2, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
tax_data <- fread("data/income.csv")
```

Before joining those two dataset, we would use groupby to find the income level based on zip code

Here, `AGI_STUB` shows the level of adjusted gross income, and `N1` shows the number of returns of each level.

The following shows the level and corresponding income range

|AGI_STUB|Range|
|-|-|
|1|\$1\~25,000|
|2|\$25,000\~\$50,000|
|3|\$50,000\~\$75,000|
|4|\$75,000\~\$100,000|
|5|\$100,000\~\$200,000|
|6|\$200,000\~|

Based on the above two columns, we generate a new column `avg_level` from 1 to 6, showing the average level of gross income in each zip code region

```{r select, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
tax_data_group <-
  tax_data %>%
  rename(zip = zipcode) %>%
  select(zip, agi_stub, N1) %>%
  mutate(n_level = agi_stub * N1) %>%
  group_by(zip) %>%
  summarise(avg_level = sum(n_level)/sum(N1))
  
```

```{r merge, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
rental_tax_data <-
  rental_data %>%
  left_join(tax_data_group, by="zip")
```

```{r assign data, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data <- rental_tax_data
```

### Filter the data

The original dataset contains 276757 data, but we just need partial data. Before we randomly pick 20000 for further analysis, we can remove rows that is lack of important factors. The criteria is as follows

* sqft (squart feet) must be non-zero
* population and the density must be non-zero
* price must be non-zero

```{r read, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_filter <- data[(data$sqft!=0 & data$Population!=0),]
data_filter <-
  data_filter %>%
  drop_na(price)
set.seed(1)
data_20000 <- sample_n(data_filter, 20000)
```

Then we drop several columns which is clearly not helpful for predicting the rental price

* link
* street_address
* full_address
* acres
* description
* zip city (duplicated data)

```{r drop, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_20000_filter <-
  data_20000 %>%
  select(-link, -street_address, -full_address, -acres, -description, -ZipCity)
```

Finally, we fill all NA with 0. The columns having NA is as follows

* pool
* dishwasher
* washer-dryer
* ac
* parking

Then we export the cleaned dataframe to csv


```{r export, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_20000_filter[is.na(data_20000_filter)] <- 0
summary(data_20000_filter)
file_path <- "data/Rental_Properties_20000.csv"
write.csv(data_20000_filter, file_path)
data_20000_filter <- fread(file_path)
# if(!file.exists(file_path)) {
#   write.csv(data_20000_filter, file_path)
# } else {
#   data_20000_filter <- fread(file_path)
# }
```

## Data Transformations and Plots

```{r plot bar, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
p<-ggplot(data=data_20000_filter, aes(x=house_type)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  xlab('Type of Property') +
  geom_bar()
p
```
```{r count, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
y <- count(data_20000_filter, house_type)
print(y)
```

Looking at the data we see that the properties we will most be evaluating will be Apartment style places with 17749 observations. Other places are lesser in number but still there. The second largest group is the Condo/Multiplex group that we are looking at with 1519 observations. Other than that the smallest group we see is the sublease or student contract group which only has 1 observation.

```{r plot2, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
p<-ggplot(data=data_20000_filter, aes(x=state)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  xlab('State in which the property is located') +
  geom_bar()
p
```

```{r count2, echo = FALSE, warning = FALSE, message = FALSE, results= 'hide'}
number_states <- data_20000_filter %>%
  group_by(state) %>% count() 
arrange(number_states, -n)
```

The data represents all states, some more than others. Texas is the most represented state with the least being Vermont at 4 listings. The sample is representative of all states in the US. We are randomly choosing 20,000 data points so this will fluctuate if we change the data seed. 


```{r  box plot, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
data_20000_filter %>% 
  ggplot(aes(x = reorder(state,price), y = price ,group=state))+
  ylim (0, 10000) + 
  xlab('State')+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Within State Variability of Rental Price")+
  ylab("Monthly rent (in $)")
```

We see the prices remaining fairly same for the start with the prices rising with states like New York, California, Florida and Massachusetts. This is evidently true for these "more expensive" places as states on the right end of the spectrum like California, and Massachusetts are known for their high per capita incomes which tends to push housing prices up. There is a lot of variation in data as well as we move to states with higher housing prices, indicated through the presence of excessive outliers on the right end of the boxplot.



# Model Training

## Dataset Preparation

To do the further model analysis, we transform some of the data. Firstly, convert the following columns from characters to binary data

* house_type
* smoking_ind
* pets_ind
* state

```{r convert chr, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_model <- data_20000_filter
data_model$house_type <- as.numeric(as.factor(data_model$house_type))
data_model$smoking_ind <- as.numeric(as.factor(data_model$smoking_ind))
data_model$pets_ind <- as.numeric(as.factor(data_model$pets_ind))
data_model$state <- as.numeric(as.factor(data_model$state))
data_model <- select(data_model, -V1)
```

Also, we need to predict the rental price which is continuous. In order to implement random forest, we transform the price to binary outcome, with the threshold of median of rental price, which is (1500).

```{r binary, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
price.median <- median(data_model$price)
data_model$price_binary <- ifelse(data_model$price < price.median, 0, 1)
```

Unlike state, the amount of unique city is far more than the state. In our sample data, we find there are 2395 unique cities, more than 1/10 of the observation. Therefore, we choose to discard this factor for our further model analysis.

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
length(unique(data_model$city))
data_model <- select(data_model, -city)
```

We split the data for choosing and validating and model

* train: 13000
* test: 5000
* validation: the rest

```{r reduce, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
set.seed(1)  # for the purpose of reporducibility
n <- nrow(data_model)
train_test.index <- sample(n, 18000)
train.index <- sample(train_test.index, 13000)
#train.index <- sample(n, 13000)
test.index <- sample(n, 19999) - (sample(n, 19999)-train_test.index) - train.index
# Split the data
n1 <- floor(13000)
n2 <- floor(5000)
set.seed(1)
idx_train <- sample(n, n1)
idx_no_train <- which(! seq(1:n) %in% idx_train)
idx_test <- sample(idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)
data.w.price.train <- data_model[idx_train,]
data.w.price.test <- data_model[idx_test,]
data.w.price.val <- data_model[idx_val,]
```

## Linear Regression

### Normal Linear Regression
In this part, we fit the linear regression model `price` vs. other variables  as `fit1` directly, then select significant variables to fit the final fit, `fit2`. 

1. Since the `description` and `price` features are useless for linear model, we drop them.
2. From the summary of fit1, we select pool, dishwasher, washer-dryer, ac, parking, state, num_beds, num_baths, house_type, sqft, Population, population density and security deposit as the variables of our final model. All the variables are significant at 0.001. The result of this model is showed as follow.
3. Training MSE: 965083.3, Testing MSE: 1304949


```{r normal LM, echo = FALSE, warning = TRUE, message = FALSE, results = 'asis'}
# data.lm.train <- select(data.w.price.train, -description)
data.lm.train <- select(data.w.price.train, -price_binary)

fit1 <- lm(price ~ ., data = data.lm.train)  # model one 
# summary(fit1) 

fit2 <- lm(price ~ pool+ dishwasher + `washer-dryer`+ ac+ parking+ state+ num_beds+ num_baths+ house_type+ sqft+ Population+ PopulationDensity + security_deposit + avg_level, data = data.lm.train)

pander(summary(fit2), caption = "summary of linear regression model")

# data.lm.test <- select(data.w.price.test, -description)

```

```{r normal LM 2, echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
data.lm.test <- select(data.w.price.test, -price_binary)

training_mse = mean((data.lm.train$price - predict.lm(fit2, data.lm.train)) ^ 2)
testing_mse = mean((data.lm.test$price - predict.lm(fit2, data.lm.test)) ^ 2)

par(mfrow=c(1,2))

plot(fit2, 1:2)
```

### LASSO Model

In this part, we introduce LASSO for a linear regression model with fewer variable since in many case not all the variables are useful for a model. So using lasso can help to reduce computation load significantly with maintaining a good result. We build the model with lasso following 3 steps.

1. Description and price_binary column are drop since they make no sense for this model.
2. `price` is set to be X and the other column of training data are set to be Y, and they are used to fit the first model, `model1`, with alpha = 1 and nfolds = 10.
3. We use LASSO to select several useful variables and refit the linear regression model, named `model2_lasso`, finding that all the vaiables are significant at 0.001, including dishwasher, number of baths, sqft, population density and security deposit.


```{r LASSO, echo = FALSE, warning = FALSE, message = FALSE, results = TRUE}
library(glmnet)

set.seed(1)
x_variables <- model.matrix(price~., data.lm.train)[,-1]
y_variable  <- data.lm.train$price

model1 <- cv.glmnet(x_variables, y_variable, nfolds = 10, alpha = 1)
plot(model1)

# cv.error.lasso=model1$cvm[model1$lambda==model1$lambda.1se]


coef.1se <- coef(model1, s="lambda.1se")
coef.1se <- coef.1se[which(coef.1se !=0),][-1]
var.1se <- rownames(as.matrix(coef.1se))

lasso_sub <- data.lm.train %>%
  select(price, var.1se)

model2_lasso <- lm(price~.,lasso_sub)
pander(summary(model2_lasso), caption = "summary of lasso regression model")


par(mfrow=c(1,2))

plot(model2_lasso, 1:2)
```

## Logistic Regression

After using linear regression models to create the predictors for specific price of rental, we use logistic regression model to predict whether one rental house or department's price is higher or lower than the median. In this part, `price_binary` that we create early is used as the result for prediction, and we build the model following 4 steps.

1. Description and price column are drop since they make no sense for this model.
2. Price_binary is set to be X and the other column of training data are set to be Y, and they are used to fit the first model, `fit.log.cv`, with alpha = 1 and nfolds = 10.
3. We use LASSO to select several useful variables and refit the logistic regression model, named `fit.logit2`, finding that `num_beds` is not significant in this model. So we drop this column and refit the model again, named `fit.logit.final`.
4.Finally, the logistic regression model includes pool, dishwasher, washer_dryer, ac, parking, state, num_baths, house_type, sqft, smoking_ind, pets_ind, PopulationDensity and security_deposit. All of the variables are significant at 0.01. Among those, ac, dishwasher and Pool play relatively important roles when the prices are setting.


```{r LogReg, echo = FALSE, warning = FALSE, message = FALSE, results = TRUE}
# data.log.train <- select(data.w.price.train, -description)
data.log.train <- select(data.w.price.train, -price)
data.log.test <- select(data.w.price.test, -price)
# data.log.train <- select(data.log.train, -acres)
names(data.log.train)[names(data.log.train) == 'washer-dryer'] <- 'washer_dryer'
names(data.log.test)[names(data.log.test) == 'washer-dryer'] <- 'washer_dryer'
X <- model.matrix(price_binary~., data.log.train)[,-1]
Y <- data.log.train$price_binary

set.seed(10) # to have same sets of K folds
fit.log.cv <- cv.glmnet(X, Y, alpha=1, family="binomial", nfolds = 10, type.measure = "deviance")  
plot(fit.log.cv)

coef.log.1se <- coef(fit.log.cv, s="lambda.1se")  
coef.log.1se <- coef.log.1se[which(coef.log.1se !=0),] 
var.log <- rownames(as.matrix(coef.log.1se))

#log_sub <- data.log.train %>%
  #select(price_binary, var.log)

fit.logit2 <- glm(Y ~ pool+dishwasher+washer_dryer+ac+parking+state+num_beds+num_baths+house_type+sqft+
                   smoking_ind+pets_ind+Population+PopulationDensity+security_deposit+avg_level, family=binomial, 
                 data = data.log.train)
# summary(fit.logit2) 

fit.logit.final<- glm(Y ~ pool+dishwasher+washer_dryer+ac+parking+state+num_baths+house_type+sqft+
                   smoking_ind+pets_ind+PopulationDensity+security_deposit+avg_level, family = binomial, 
                 data = data.log.train)
# summary(fit.logit.final) 
pander(Anova(fit.logit.final), caption = "summary of logistic regression model (binary classification)")

```

After We get the final model of logistic regression, we predict the result in test dataset and compute the confusion matrix. From the confusion matrix, we find the misclassification error is 0.2156, which is a little bit higher.

However, after we integrate the second dataset, the misclassification error drops to 0.1922, indicating that the income would improve the final result.

```{r LogRegPrediction, echo = FALSE, warning = FALSE, message = FALSE, results = 'hide'}
# This code chunk cannot be compiled while knitting to PDF (because of dummy function)
predict.logReg <- predict(fit.logit.final, data.log.test, type="response")
threshold <- 0.5
diff_col <- ifelse(predict.logReg >= threshold, 1, 0)
cm <- table(diff_col, data.log.test$price_binary)
log.mce <- (cm[1,2]+cm[2,1])/sum(cm)
log.mce
```

## Random Forest

In random forest model, we just need the binary data of the price, so we remove `price` column, and we need to rename the column `washer-dryer` for the random forest package

```{r RF, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data.rf.train <- select(data.w.price.train, -price)
data.rf.train <- 
  data.rf.train %>%
  select(price_binary, everything())
data.rf.test <- select(data.w.price.test, -price)
data.rf.test <- 
  data.rf.test %>%
  select(price_binary, everything())
names(data.rf.train)[names(data.rf.train) == 'washer-dryer'] <- 'washer_dryer'
names(data.rf.test)[names(data.rf.test) == 'washer-dryer'] <- 'washer_dryer'
```

### Tune Hyperparameter

Firstly, we use OOB to find the testing error for given parameter, and we choose mtry from 1 to 10, with 150 tree.

```{r tune rf hyperparameter, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
set.seed(1)
rf.error.p <- 1:10
for (p in 1:10){
  fileName <- paste("data/rf_tune_", p, "_150.RData", sep = "")
  if(!file.exists(fileName)) {
    fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=p, ntree=150)
    save(fit.rf.train, file = fileName)
  } else {
    load(fileName)
  }
  rf.error.p[p] <- fit.rf.train$mse[150]
}
plot(1:10, rf.error.p, pch=16,
     main = "Testing errors of mtry with 150 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
# line(1:10, rf.error.p)
```

According to the above plot, we choose 3 as mtry based on elbow rule.

Then we set ntree to be 500, to find the optimal number of tree.

```{r given mtry 15, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
if(!file.exists("data/rf_model_3_500.RData")) {
  fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=3, ntree=500)
  save(fit.rf.train, file = "data/rf_model_3_500.RData")
} else {
  load("data/rf_model_3_500.RData")
}
plot(fit.rf.train, main = "Testing errors of trees with 3 mtry")
```

Also based on the above plot, we choose 100 as ntree

### PCA

Apply PCA to dataset would help us to remove correlated data and speed up the runtime of the program. However, in this analysis, the runtime to train 20000 data is fast enough (about several minutes). Therefore, we decide not to implement PCA here to keep the information in the dataset.

### Performance of Random Forest Model

Finally, given fixed mtry and ntree, we can compute the testing error of our random forest model. After we do prediction on the testing dataset, we set the threshold as 0.5. If the result is greater than 0.5, we would categorize the result as 1. On the other hand, result less than 0.5 is 0. Finally we compte the error to be 0.082

```{r fit rf, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
set.seed(1)
if(!file.exists("data/rf_model_4_100.RData")) {
  fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=4, ntree=100)
  save(fit.rf.train, file = "data/rf_model_4_100.RData")
} else {
  load("data/rf_model_4_100.RData")
}
plot(fit.rf.train, main = "Random Forest Model with 4 mtry and 100 ntree")
```

```{r calculate rf error, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
# This code chunk cannot be compiled while knitting to PDF (because of dummy function)
# predict.rf <- predict(fit.rf.train, newdata=data.rf.test)
# diff_col <- ifelse(abs(data.rf.test$price_binary - predict.rf) >=0.5, 1, 0)
# mean(diff_col)
```

We draw the importance of variables from the final random forest model. Here, we can see that security deposit is the most important factor, and the area is the second important factor. It is reasonable that those two factors are highly correlated to the rental price. As for the `avg_level`, it is also regarded as a crucial factor, indicating that the financial condition actually related to the rental price. Surprisingly, the facilities of the house is not important compared to the security deposit, area, localtion and the level of income.

```{r importance, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
varImpPlot(fit.rf.train)
```

# Conclusion

In conclusion, various factors affect rental prices beyond the simple square feet and location. The various micro-factors like availability of a pool, washer-dryer, dishwasher, etc., play an essential role and shouldn't be forgotten when considering a housing rental price. Our data is most applicable to Apartment and Condo/Multiplex forms of apartments. However, with repeated testing, our results can be extended to House, Townhome, Single Room and Sublease, or Student contract form of rentals. There was also considerable variability in the state where the houses are located, with states like New York being more expensive than mid-western states or southern states. The per capita incomes can also explain this in these places, drastically differing. We also ran a random forest model to predict the values with reasonable accuracy. We find that random forest is the best model to predict housing prices, combined with simple logistic regression.

