---
title: "final-project"
author:
- Pragyat Agrawal
- Ruxuan Ji
- Meng-Chuan Chang
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
  html_document:
    toc: yes
    theme: united
    toc_depth: 3
    number_sections: yes
  word_document:
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T, cache.lazy = FALSE) # notice cache=T here
knitr::opts_chunk$set(fig.height=4, fig.width=7, fig.align = 'center', warning = F)

if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(data.table, dplyr, tidyverse, xtable, ggplot2, randomForest, tfdatasets, keras)
```

# Executive Summary

## Background

The US rental market has been growing rapidly growing over time, making it one of the most sought after areas for investment. Be it from small home owners to big private equity firms, everyone seems to be after housing, expecting the values of these houses to rise and supplement their income by renting these places. Many people consider that location is the "only important" factor responsible for a house's value and the rent that can be expected of it, but this is far from the truth. There are a lot of other factors that need to be considered for determining housing valuations as we see a huge variation in prices in houses located in the same vicinity. There must be something about these houses which is causing such a big price change. Hence we will be analyzing the data related to US Rental Listings in Summer of 2021, to find which of these factors, which consist of many in-house amenity components, impacts housing values the most. 

These amenities range from simple aka micro-factors like the availability of Pools and Dishwashers in the house to major aka macro-factors i.e. cities. The data will also give us the opportunity to find in which cities are these factors playing the most impact. With over 27,000 values for each predictor in our data, we have a sufficient sample size to make a reasonable conclusion regarding the price of these summer rentals. 

Graph: Rental vacancy rates in the United States from 2000 to 2021, by region (Source: Statista.com) 

This shows how the US housing market has been more sought after year by year, making housing a form of valuable investment. This increase in demand has already pushed up the prices. 
```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("image/rental-vacancy-rates.png")
```

## Description of Data

The data is gathered from Kaggle, a huge repository of community published code and data. This data was pulled from Rentler.com on 7/12/2021, 8/12/2021, and 9/6/2021, and population density data was scraped by zip code from mapszipcode.com on 7/12/2021. The pull from Rentler.com resulted in 4 CSV files which included the main rental listing, the list of amenities, the list of lease terms, and a list of who was responsible to pay each utility. Many of the variables that were sparsely populated were dropped before denormalizing the dataset. The rental listing information was joined with the population and population density information from mapszipcode.com (Source: Kaggle). Many of the data columns in the dataset are embedded in binary format with 0 representing the absence of the predictor attribute while 1, shows that the attribute is present. The data file is massive at around half a Gigabyte. Working with this size of data, we would have to use EDA or take a subset of the dataset for R to run effectively and not crash over the large size of the data. We will explore this idea in later sections. For now, the data is sufficient for analysis.

Our response variable is PRICE which represents the monthly price for the particular summer listing on rental.com.


\begin{table}[!h]
\centering
\caption{Variables and their descriptions}
\begin{tabular}{cc}
\hline 
Variable & Description\tabularnewline
\hline 
V1 & Numbering for the house number we are looking at (form of house identification)\tabularnewline
pool & Binary data to show if pool exists in this house (1) or not (0)\tabularnewline
dishwasher & Binary data to show if dishwasher exists in the house (1) or not (0)\tabularnewline
washer-dryer & Binary data to show if washer-dryer exists in the house(1) or not (0)\tabularnewline
ac & Binary data to show if air conditioning exists in the house (1) or not (0)\tabularnewline
parking & Binary data to show if Parking exists in the house (1) or not (0)\tabularnewline
zip & Zip code of the property\tabularnewline
price & Monthly rent price for the property\tabularnewline
city & City where the property is located\tabularnewline
num\_beds & Number of beds in the property\tabularnewline
num\_baths & Number of baths in the property\tabularnewline
house\_type & Type of house we are looking at\tabularnewline
sqft & Square Feet in the property\tabularnewline
smoking\_ind & Does the rental allow smoking (Yes/No) \tabularnewline
pets\_ind & Does the rental allow pets (Yes/No)\tabularnewline
acres & Number of acres rental includes\tabularnewline
description & 4000 character listing description of the rental\tabularnewline
ZipCity & Primary city for the zip code\tabularnewline
Population & Population in the zip code\tabularnewline
PopulationDensity & Population density per square mile for zip code\tabularnewline
security\_deposit & Security deposit required\tabularnewline
\hline 
\end{tabular}
\label{lyxtab2}
\end{table}

## Goal

The goal of this study is to analyze the most important factors affecting the housing prices in the US. We will be using the variables in the dataset to do so. In our preliminary market analysis, we found that housing prices are determined by many factors and hence we will try to ascertain factors which have a significant impact on housing prices. Our goal is to build a model that will give us the value added or subtracted from a house with/without the presence of a variable factor. This observation will benefit people who are looking to rent properties in the US and can help them get a better value for the kind of place they may be looking for. 

## Summary of Findings

## Issues and Limitations

The biggest issue we initially faced was with respect to the file size which turned out to be quiet massive even for R Studio. The raw data we started with was half a gigabyte big which turned out to be very massive for any for of extrapolation. Hence we had to shorten the data out.......

# Exploratory Data Analysis

## Data Preprocessing/Cleaning

### Read the data

Firstly, we read the data from Kaggle - [US Rental Listings Summer 2021](https://www.kaggle.com/datasets/elizabethveillon/us-rental-listings-summer-2021)

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
rental_data <- fread("data/Rental_Properties.csv")
summary(rental_data)
```

Then we read SOI Tax statistics in 2019 from [IRS](https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2019-zip-code-data-soi)

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
tax_data <- fread("data/income.csv")
```

Before joining those two dataset, we would use groupby to find the income level based on zip code

Here, `AGI_STUB` shows the level of adjusted gross income, and `N1` shows the number of returns of each level.

The following shows the level and corresponding income range

|AGI_STUB|Range|
|-|-|
|1|\$1\~25,000|
|2|\$25,000\~\$50,000|
|3|\$50,000\~\$75,000|
|4|\$75,000\~\$100,000|
|5|\$100,000\~\$200,000|
|6|\$200,000\~|

Based on the above two columns, we generate a new column `avg_level` from 1 to 6, showing the average level of gross income in each zip code region

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
tax_data_group <-
  tax_data %>%
  rename(zip = zipcode) %>%
  select(zip, agi_stub, N1) %>%
  mutate(n_level = agi_stub * N1) %>%
  group_by(zip) %>%
  summarise(avg_level = sum(n_level)/sum(N1))
  
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
rental_tax_data <-
  rental_data %>%
  left_join(tax_data_group, by="zip")
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data <- rental_tax_data
```

### Filter the data

The original dataset contains 276757 data, but we just need partial data. Before we randomly pick 20000 for further analysis, we can remove rows that is lack of important factors. The criteria is as follows

* sqft (squart feet) must be non-zero
* population and the density must be non-zero
* price must be non-zero

```{r read, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_filter <- data[(data$sqft!=0 & data$Population!=0),]
data_filter <-
  data_filter %>%
  drop_na(price)
set.seed(1)
data_20000 <- sample_n(data_filter, 20000)
```

Then we drop several columns which is clearly not helpful for predicting the rental price

* link
* street_address
* full_address
* acres
* description
* zip city (duplicated data)

```{r drop, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_20000_filter <-
  data_20000 %>%
  select(-link, -street_address, -full_address, -acres, -description, -ZipCity)
```

Finally, we fill all NA with 0. The columns having NA is as follows

* pool
* dishwasher
* washer-dryer
* ac
* parking

Then we export the cleaned dataframe to csv


```{r export, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_20000_filter[is.na(data_20000_filter)] <- 0
summary(data_20000_filter)

file_path <- "data/Rental_Properties_20000.csv"
write.csv(data_20000_filter, file_path)
data_20000_filter <- fread(file_path)

# if(!file.exists(file_path)) {
#   write.csv(data_20000_filter, file_path)
# } else {
#   data_20000_filter <- fread(file_path)
# }
```

## Data Transformations and Plots

```{r, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
p<-ggplot(data=data_20000_filter, aes(x=house_type)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  xlab('Type of Property') +
  geom_bar()
p
```
```{r, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
y <- count(data_20000_filter, house_type)
print(y)
```

Looking at the data we see that the properties we will most be evaluating will be Apartment style places with 17749 observations. Other places are lesser in number but still there. The second largest group is the Condo/Multiplex group that we are looking at with 1519 observations. Other than that the smallest group we see is the sublease or student contract group which only has 1 observation.

```{r, echo = FALSE, warning = FALSE, message = FALSE, results= 'asis'}
p<-ggplot(data=data_20000_filter, aes(x=state)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  xlab('State in which the property is located') +
  geom_bar()
p
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, results= 'hide'}
number_states <- data_20000_filter %>%
  group_by(state) %>% count() 

arrange(number_states, -n)
```

The data represents all states, some more than others. Texas is the most represented state with the least being Vermont at 4 listings. The sample is representative of all states in the US. We are randomly choosing 20,000 data points so this will fluctuate if we change the data seed. 


```{r, warning = FALSE, message = FALSE, results= 'asis'}
data_20000_filter %>% 
  ggplot(aes(x = reorder(state,price), y = price ,group=state))+
  ylim (0, 10000) + 
  xlab('State')+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  ggtitle("Within State Variability of Rental Price")+
  ylab("Monthly rent (in $)")
```

We see the prices remaining fairly same for the start with the prices rising with states like New York, California, Florida and Massachusetts. This is evidently true for these "more expensive" places as states on the right end of the spectrum like California, and Massachusetts are known for their high per capita incomes which tends to push housing prices up. There is a lot of variation in data as well as we move to states with higher housing prices, indicated through the presence of excessive outliers on the right end of the boxplot.

# Model Training

To do the further model analysis, we transform some of the data. Firstly, convert the following columns from characters to binary data

* house_type
* smoking_ind
* pets_ind
* state

```{r convert chr, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data_model <- data_20000_filter
data_model$house_type <- as.numeric(as.factor(data_model$house_type))
data_model$smoking_ind <- as.numeric(as.factor(data_model$smoking_ind))
data_model$pets_ind <- as.numeric(as.factor(data_model$pets_ind))
data_model$state <- as.numeric(as.factor(data_model$state))
data_model <- select(data_model, -V1)
```

Also, we need to predict the rental price which is continuous. In order to implement random forest, we transform the price to binary outcome, with the threshold of median of rental price, which is (1500).

```{r binary, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
price.median <- median(data_model$price)
data_model$price_binary <- ifelse(data_model$price < price.median, 0, 1)
```

Unlike state, the amount of unique city is far more than the state. In our sample data, we find there are 2395 unique cities, more than 1/10 of the observation. Therefore, we choose to discard this factor for our further model analysis.

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
length(unique(data_model$city))
data_model <- select(data_model, -city)
```

# Model Training

Firstly, we split the data for choosing and validating and model

* train: 13000
* test: 5000
* validation: the rest

```{r reduce, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
set.seed(1)  # for the purpose of reporducibility
n <- nrow(data_model)
train_test.index <- sample(n, 18000)
train.index <- sample(train_test.index, 13000)
#train.index <- sample(n, 13000)
test.index <- sample(n, 19999) - (sample(n, 19999)-train_test.index) - train.index

# Split the data

n1 <- floor(13000)
n2 <- floor(5000)
set.seed(1)
idx_train <- sample(n, n1)
idx_no_train <- which(! seq(1:n) %in% idx_train)
idx_test <- sample(idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)

data.w.price.train <- data_model[idx_train,]
data.w.price.test <- data_model[idx_test,]
data.w.price.val <- data_model[idx_val,]
```

## Random Forest

In random forest model, we just need the binary data of the price, so we remove `price` column, and we need to rename the column `washer-dryer` since the random forest package

```{r, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
data.rf.train <- select(data.w.price.train, -price)
data.rf.train <- 
  data.rf.train %>%
  select(price_binary, everything())

data.rf.test <- select(data.w.price.test, -price)
data.rf.test <- 
  data.rf.test %>%
  select(price_binary, everything())

names(data.rf.train)[names(data.rf.train) == 'washer-dryer'] <- 'washer_dryer'
names(data.rf.test)[names(data.rf.test) == 'washer-dryer'] <- 'washer_dryer'
```

### Tune Hyperparameter

Firstly, we use OOB to find the testing error for given parameter, and we choose mtry from 1 to 10, with 150 tree.

```{r tune rf hyperparameter, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}

set.seed(1)
rf.error.p <- 1:10
for (p in 1:10){
  fileName <- paste("data/rf_tune_", p, "_150.RData", sep = "")
  if(!file.exists(fileName)) {
    fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=p, ntree=150)
    save(fit.rf.train, file = fileName)
  } else {
    load(fileName)
  }
  rf.error.p[p] <- fit.rf.train$mse[150]
}

plot(1:10, rf.error.p, pch=16,
     main = "Testing errors of mtry with 150 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
# line(1:10, rf.error.p)

```

According to the above plot, we choose 3 as mtry based on elbow rule.

Then we set ntree to be 500, to find the optimal number of tree.

```{r given mtry 15, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}

if(!file.exists("data/rf_model_3_500.RData")) {
  fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=3, ntree=500)
  save(fit.rf.train, file = "data/rf_model_3_500.RData")
} else {
  load("data/rf_model_3_500.RData")
}
plot(fit.rf.train, main = "Testing errors of trees with 3 mtry")
```

Also based on the above plot, we choose 100 as ntree

### PCA

Apply PCA to dataset would help us to remove correlated data and speed up the runtime of the program. However, in this analysis, the runtime to train 20000 data is fast enough (about several minutes). Therefore, we decide not to implement PCA here to keep the information in the dataset.

### Performance of Random Forest Model

Finally, given fixed mtry and ntree, we can compute the testing error of our random forest model. After we do prediction on the testing dataset, we set the threshold as 0.5. If the result is greater than 0.5, we would categorize the result as 1. On the other hand, result less than 0.5 is 0. Finally we compte the error to be 0.082

```{r fit rf, echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
set.seed(1)

if(!file.exists("data/rf_model_4_100.RData")) {
  fit.rf.train <- randomForest(price_binary~., data=data.rf.train, mtry=4, ntree=100)
  save(fit.rf.train, file = "data/rf_model_4_100.RData")
} else {
  load("data/rf_model_4_100.RData")
}

plot(fit.rf.train, main = "Random Forest Model with 4 mtry and 100 ntree")
```

```{r calculate rf error, echo = FALSE, warning = FALSE, message = FALSE, results='hide'}
# This code chunk cannot be compiled while knitting to PDF (because of dummy function)
# predict.rf <- predict(fit.rf.train, newdata=data.rf.test)
# diff_col <- ifelse(abs(data.rf.test$price_binary - predict.rf) >=0.5, 1, 0)
# mean(diff_col)
```

# Performance Analysis

# Conclusion

